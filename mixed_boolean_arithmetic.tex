\section{Mixed Boolean Arithmetic}
\subsection{Introduction to Mixed Boolean Arithmetic}
In computer programming, numbers are represented
not in decimal but in binary with a limited number of bits allocated to the 
variable. That means each number is represented in memory as a bit-vector of
the form $\{0,1\}^n$, or $\B^n$ where $\B$ is for binary, and where $n$ is the ``word size'', that is, the maximum 
size of a given number. To compute the decimal value of a given bit vector $v \in \B^n$, 
take the following sum with $v_i$ representing the $i$th bit of $v$:
\begin{align*}
    v = \sum_{i = 0}^{n-1} v_i \cdot 2^{n-i-1}
\end{align*}
For our purposes, that means the most significant (largest value) bit is the topmost
or first element of $v$. This means that, for example, the number $2$ in $\B^4$ would be represented as 
\begin{align*}
    v = \begin{bmatrix}
        0 \\ 0 \\1 \\0
    \end{bmatrix} = \sum_{i=0}^{3}v_i \cdot 2^{4-i-1} = 2^3 \cdot 0 + 2^2 \cdot 0 + 2^1 \cdot 1 + 2^0 \cdot 0 = 2
\end{align*}
This is just a way of interpreting the bit vector. In other words, we could also say that 
if the word size is 4, then $2 = 0010$ in binary. All of these expressions are
equivalent ways of representing the same value.
\par Using these bit-vector numbers, we can use different
operations based on how we treat them, I.E. using bitwise operators such as
$\ll$ and $\gg$ for shifting bits right and left, $\land$ and $\lor$ (bitwise AND and OR), $\neg$ (bitwise NOT), 
and $\oplus$ (bitwise XOR), or using traditional arithmetic operators $+, -, \times,$ etc.
\par When using bitwise operators, it is certainly more clear to treat the 
number as a bit-vector, while when using traditional arithmetic operators
we get the same result by taking the result of normal arithmetic over the modular ring 
($\Z/2^n\Z$). This is because when doing arithmetic on bit-vectors of a set size, 
any time the result we get is larger than can be represented in $n$ bits, the result
is taken modulo $2^n$. For the purposes of our paper, we will intermingle using the 
bit vector and decimal representations of a number, as this ultimately gives us the 
same result and allows us to mix bitwise and arithmetic operators in the same
expression.
\par As an aside for those less comfortable with the bitwise operators defined above, 
this is an example of a truth table, which shows the given values of binary 1-bit
numbers on the left, and in the next columns the corresponding value of an 
expression including some of the operators we've defined above.
\begin{align*}
    \begin{tabular}{c|c|c|c|c|c}
        $x$ & $y$ & $\neg x$ & $x \oplus y$ & $x \land y$ & $x \lor y$\\
        \hline
        0 & 0 & 1 & 0 & 0 & 0\\
        1 & 0 & 0 & 1 & 0 & 1\\
        0 & 1 & 1 & 1 & 0 & 1\\
        1 & 1 & 0 & 0 & 1 & 0\\
    \end{tabular}
\end{align*}
\thm{Definition 1} {
    A bitwise expression $e_j$ is defined as an expression constituted by
    bitwise operators $\land, \lor, \oplus,$ or $\neg$ on some set of variables
    $(x_1, \dots, x_t)$. For example, consider some expressions in the vector space $\B^4$ consisting
    of bit vectors with $4$ bits, with the following:
    \begin{align*}
        x &= \begin{bmatrix}
        0 & 1 & 1 & 1
    \end{bmatrix}^T = 7 & y &= \begin{bmatrix}
        1 & 0 & 1 & 0
    \end{bmatrix}^T = 10
    \\
    x \lor y &= \begin{bmatrix}
        1 & 1 & 1 & 1
    \end{bmatrix}^T = 15
             & x \oplus y &= \begin{bmatrix}
                  1 & 1 & 0 & 1
              \end{bmatrix}^T = 13
    \\
        x \land y &= 
        \begin{bmatrix}
            0 & 0 & 1 & 0
        \end{bmatrix}^T = 2 & \neg y &= \begin{bmatrix}
        0 & 1 & 0 & 1
        \end{bmatrix}^T
        = 5
    \end{align*}
}
Consider also doing normal arithmetic operations on bit vectors:
\begin{align*}
x + y &= \begin{bmatrix}
0 & 0 & 0 & 1 
\end{bmatrix}^T = 1 = 17 \mod 2^4 
\end{align*}
\par In this example, a bit should be carried to
the 5th position from the right, but it gets truncated as we're working in $\B^4$, resulting
in $x + y$ being smaller then $x$ or $y$. 
This is why we get the same result as taking the addition in
$\Z/(2^n\Z)$.
\par Now, to formally define a mixed boolean arithmetic expression:
\thm{Definition 2} {Let $n,s,t \in \N$, where $n$ is the number of bits in a bit vector, 
$s$ is the number of bitwise expressions, and $t$ is the number of 
inputs to the expressions.
Let $x_i$ be a variable over $\B^n$ for
$i = 1,...,t$, and $e_j : (\B^n)^t \rightarrow \B^n$ be a bitwise expression for
$j = 1,...,s$. Let $a_j\in \B^n$ be constant coefficients for $j = 1,...,s$.
Then, a linear MBA (mixed boolean arithmetic) expression $e$ is an expression that can be written as follows:
\begin{align*}
    e(x_1, ..., x_t) = \sum_{j=1}^s a_j e_j(x_1,...,x_t)
\end{align*}
}
\par Since we've now defined the the essential operators we have in mixed-boolean 
arithmetic
expressions, we can imagine several situations where a programmer might use expressions 
that they would rather keep secret. In cryptography-relevant code, we may use any
number of cryptographic methods that are composed of MBA 
expressions
and wish to keep the exact constituent operations hidden 
to dissuade attackers. For example, let's say we are working on a 
program that uses the following function $F : (\B^{32})^3 \rightarrow \B^{32}$ as part
of a top-secret cryptography algorithm:
\begin{align*}
    F(x, y, z) = 22(x - y + \neg z)
\end{align*}
This means  that $F$ takes three bit-vectors of size 32 bits, $x, y, z \in \B^{32}$, 
and returns the bit vector equivalent to that given expression. 
(Note that we can also use 2's complement representation for values
which we want to be signed, which means that for some
bit vector $v \in \B^n$, $-v = \neg v + 1$, and a number is negative if and only if
its first bit is set to 1. This allows us to represent negative
numbers using only 0's and 1's in a bit vector. Also note here that the results of
a bitwise operator do {\itshape not change} using 2's complement, only the 
interpretation of the result. The same goes for arithmetic operators
the underlying bit vector representation.)
By the definition we've given above, we know that $F$ is an MBA expression. 
In the next section, 
we will take some tools from linear algebra and use them to obfuscate $F$.
\subsection{Mixed Boolean Arithmetic in Linear Algebra}
Now, there are two ways to re-write an MBA expression as something more complex:
re-writing and inserting identities. Inserting identities means to tag onto
the expression some MBA expression that is always 0, and re-writing means to 
replace an expression with one that is equivalent. First, let's look at methods
of generating and proving identities. There is one critically important theorem 
that will allow us to generate arbitrary MBA identities:
\thm{Theorem 1}{Let e be a linear MBA expression with the same variable definitions
as in Definition 2. Then let $(v_{0,j},...,v_{2^t-1, j})^T$ be the column vector
of the truth table of the Boolean expression from $e_j$, and let 
$A = (v_{i,j})_{2^t \times s}$ be the $\B$-matrix of those truth tables over ($\Z/2^n\Z$).
Let $Y_{s \times 1} = (y_0, \dots, y_{s-1})^T$ be a vector of $s$ variables over
$(\Z/2^n\Z)$. Then $e = 0$ if and only if $AY = 0$ has a solution over $(\Z/2^n\Z)$.}
\pf{
    First, we prove that $e = 0 \implies AY = 0$ has a solution over $(\Z/2^n\Z)$.
    Consider some MBA $e : e = 0$. Then $Y = (a_0, a_1, \dots, a_j)^T$ is 
    clearly a solution to $AY = 0$, since we know that $e = 0$ and thus
    taking the constants from $e$ and multiplying them by the truth tables of 
    the constituent expressions of $e$ will be zero.
    \par Then, we need to prove that $AY = 0 \implies e = 0$. Consider some 
    solution $Y = (y_0, ..., y_j)^T : AY = 0$. Let $y_{i,j}$ be the $i$th bit
    of $y_j$. Then, by Zhou {\itshape et al}'s ``Information hiding in software
    with mixed boolean-arithmetic transforms,'' we have the following:
    \begin{align*}
        e = \sum_{j=0}^{s-1}
    \end{align*}
}

\par What this theorem essentially allows us to do is build any matrix $A$
composed of column vectors that are truth table values, then solve the 
equation $Ax = 0$ for some vector $x$, and we'll have an identity MBA expression.
\par For example, consider that we wanted to compose an MBA identitity consisting 
of the following expressions:
\begin{align*}
    f_0(x,y) &= x\\
    f_1(x,y) &= y\\
    f_2(x,y) &= x \oplus y\\
    f_3(x,y) &= x \lor (\neg y)\\
    f_4(x,y) &= -1
\end{align*}
Then, we take the truth tables for each of these expressions and form them
into a column vector, then compose them into a matrix. In this case, we get 
the following:
\begin{align*}
    F = \begin{bmatrix}
        0 & 0 & 0 & 1 & 1\\
        0 & 1 & 1 & 0 & 1\\
        1 & 0 & 1 & 1 & 1\\
        1 & 1 & 0 & 1 & 1\\
    \end{bmatrix}
\end{align*}
This matrix is composed of truth table values for the following functions (where 
$f_i$ represents the function in the $i$'th column) :
(Where we've defined -1 to be all 1's as in two's complement representation).
Then, solving for $Ax = 0$, we get:
\begin{align*}
    x = \begin{bmatrix}
    1\\-1\\-1\\-2\\2
    \end{bmatrix}
\end{align*}
So, plugging in our expressions from the functions the matrix was composed of, 
we get the following equation:
\begin{align*}
    x - y - (x \oplus y) -2(x \lor (\neg y)) + 2 =0
\end{align*}

\noindent
\begin{minipage}{.5\textwidth}
    \begin{small}
    \begin{framed}
        \begin{verbatim}
def F(x: int, y: int,
      z: int) -> int:
    return (x - y + ~z)
        \end{verbatim}
    \end{framed}
    \end{small}
\end{minipage}% This must go next to `\end{minipage}`
\begin{minipage}{.5\textwidth}
    \begin{framed}
        \begin{small}
        \begin{verbatim}
def G(x: int, y: int,
      z: int) -> int:
    return (x - y + ~z)
        \end{verbatim}
        \end{small}
    \end{framed}
\end{minipage}
{\color{blue} TODO: Prove that this holds for arbitrarily sized bit-vectors, show other 
examples, }
\subsection{Obfuscating Mixed Boolean Arithmetic}
Now, using the method in the previous section to generate MBA identities works well,
but we can also use it to find MBA equalities that allow us to rewrite our original
expression. Take the first example:
\begin{align*} 
    x - y - (x \oplus y) -2(x \lor (\neg y)) + 2 = 0\\
    \implies x - y = (x \oplus y) + 2(x \lor (\neg y)) - 2
\end{align*}
Then, in any MBA expression that contains $(x - y)$, we can replace it with
the expression on the right. Since we can generate arbitrary MBA identities, 
we can compose any matrix that includes some of the operations in whichever
MBA expression we'd like to obfuscate, and then use the resulting equality
to replace expressions with equivalent ones as long as we'd like to! 
