\section{Mixed Boolean Arithmetic}
\subsection{Introduction to Mixed Boolean Arithmetic}
In computer programming, numbers are represented
not in decimal but in binary with a limited number of bits allocated to the
variable. That means each number is represented in memory as a bit-vector of
the form $\{0,1\}^n$, or $\B^n$ where $\B$ is for binary, and where $n$ is the ``word size'', that is, the maximum
size of a given number. To compute the decimal value of a given bit vector $v \in \B^n$,
take the following sum with $v_i$ representing the $i$th bit of $v$:
\begin{align*}
    v = \sum_{i = 0}^{n-1} v_i \cdot 2^{i}
\end{align*}
For our purposes, that means the most significant (largest value) bit is the topmost
or first element of $v$ and we say that $v_0$ is the bottom-most element. Note
that
this is different from how vectors would normally be indexed in linear algebra, but we
do this to make it simpler.
This means that, for example, the number $2$ in $\B^4$ would be represented as
\begin{align*}
    v = \begin{bmatrix}
        0 \\ 0 \\1 \\0
    \end{bmatrix} = \sum_{i=0}^{3}v_i \cdot 2^{i} = 2^0 \cdot 0 + 2^1 \cdot 1 + 2^2 \cdot 0 + 2^3 \cdot 0 = 2
\end{align*}
This is just a way of interpreting the bit vector. In other words, we could also say that
if the word size is 4, then $2 = 0010$ in binary. All of these expressions are
equivalent ways of representing the same value.
\par Using these bit-vector numbers, we can use different
operations based on how we treat them, i.e. using bitwise operators such as
$\ll$ and $\gg$ for shifting bits right and left, $\land$ and $\lor$ (bitwise AND and OR), $\neg$ (bitwise NOT),
and $\oplus$ (bitwise XOR), or using traditional arithmetic operators $+, -, \times,$ etc.
\par When using bitwise operators, it is certainly more clear to treat the
number as a bit-vector, while when using traditional arithmetic operators
we get the same result by taking the result of normal arithmetic over the modular ring
($\Z/2^n\Z$). This is because when doing arithmetic on bit-vectors of a set size,
any time the result we get is larger than can be represented in $n$ bits, the result
is taken modulo $2^n$. For the purposes of our paper, we will intermingle using the
bit vector and decimal representations of a number, as this ultimately gives us the
same result and allows us to mix bitwise and arithmetic operators in the same
expression.
\par As an aside for those less comfortable with the bitwise operators defined above,
this is an example of a truth table, which shows the given values of binary 1-bit
numbers on the left, and in the next columns the corresponding value of an
expression including some of the operators we've defined above.
\begin{align*}
    \begin{tabular}{c|c|c|c|c|c}
        $x$ & $y$ & $\neg x$ & $x \oplus y$ & $x \land y$ & $x \lor y$\\
        \hline
        0 & 0 & 1 & 0 & 0 & 0\\
        1 & 0 & 0 & 1 & 0 & 1\\
        0 & 1 & 1 & 1 & 0 & 1\\
        1 & 1 & 0 & 0 & 1 & 0\\
    \end{tabular}
\end{align*}
\thm{Definition 1} {
    A bitwise expression $e_j$ is defined as an expression constituted by
    bitwise operators $\land, \lor, \oplus,$ or $\neg$ on some set of variables
    $(x_1, \dots, x_t)$. For example, consider some expressions in the vector space $\B^4$ consisting
    of bit vectors with $4$ bits, with the following:
    \begin{align*}
        x &= \begin{bmatrix}
        0 & 1 & 1 & 1
    \end{bmatrix}^T = 7 & y &= \begin{bmatrix}
        1 & 0 & 1 & 0
    \end{bmatrix}^T = 10
    \\
    x \lor y &= \begin{bmatrix}
        1 & 1 & 1 & 1
    \end{bmatrix}^T = 15
             & x \oplus y &= \begin{bmatrix}
                  1 & 1 & 0 & 1
              \end{bmatrix}^T = 13
    \\
        x \land y &=
        \begin{bmatrix}
            0 & 0 & 1 & 0
        \end{bmatrix}^T = 2 & \neg y &= \begin{bmatrix}
        0 & 1 & 0 & 1
        \end{bmatrix}^T
        = 5
    \end{align*}
}
Note that there are other bitwise operators such as left and right shift, but for our
purposes we will only use the four in this definition. Consider also doing normal arithmetic operations on bit vectors:
\begin{align*}
x + y &= \begin{bmatrix}
0 & 0 & 0 & 1
\end{bmatrix}^T = 1 = 17 \mod 2^4
\end{align*}
\par In this example, a bit should be carried to
the 5th position from the right, but it gets truncated as we're working in $\B^4$, resulting
in $x + y$ being smaller then $x$ or $y$.
This is why we get the same result as taking the addition in
$\Z/(2^n\Z)$.
\par Now, to formally define a mixed boolean arithmetic expression:
\thm{Definition 2} {Let $n,s,t \in \N$, where $n$ is the number of bits in a bit vector,
$s$ is the number of bitwise expressions, and $t$ is the number of
inputs to the expressions.
Let $x_i$ be a variable over $\B^n$ for
$i = 1,...,t$, and $e_j : (\B^n)^t \rightarrow \B^n$ be a bitwise expression for
$j = 0,...,s-1$. Let $a_j\in \B^n$ be constant coefficients.
Then, a linear MBA (mixed boolean arithmetic) expression $e$ is an expression that can be written as follows:
\begin{align*}
    e(x_1, ..., x_t) = \sum_{j=0}^{s-1} a_j e_j(x_1,...,x_t)
\end{align*}
}
\par Since we've now defined the essential operators we have in mixed-boolean
arithmetic
expressions, we can imagine several situations where a programmer might use expressions
that they would rather keep secret. In cryptography-relevant code, we may use any
number of cryptographic methods that are composed of MBA
expressions
and wish to keep the exact constituent operations hidden
to dissuade attackers. For example, let's say we are working on a
program that uses the following function $F : (\B^{32})^3 \rightarrow \B^{32}$ as part
of a top-secret cryptography algorithm:
\begin{align*}
    F(x, y, z) = 22(x - y + \neg z)
\end{align*}
This means  that $F$ takes three bit-vectors of size 32 bits, $x, y, z \in \B^{32}$,
and returns the bit vector equivalent to that given expression.
(We can use 2's complement representation for values which we want to be negative,
which means that for some bit vector $v \in \B^n$, $-v = \neg v + 1$,
and a number is negative if and only if its first bit is set to 1.
This allows us to represent negative numbers using only 0's and 1's in a bit vector.
Also note here that the results of a bitwise operator do {\itshape not change}
using 2's complement, only the  interpretation of the result. The same goes for
arithmetic operators the underlying bit vector representation.)
By the definition we've given above, we know that $F$ is an MBA expression.
In the next section,
we will use some tools from linear algebra to obfuscate $F$.
\subsection{Mixed Boolean Arithmetic in Linear Algebra}
Now, there are two ways to re-write an MBA expression as something more complex:
re-writing and inserting identities. Inserting identities means to tag onto
the expression some MBA expression that is always 0, and re-writing means to
replace an expression with one that is equivalent. First, let's look at methods
of generating and proving identities. There is one critically important theorem
that will allow us to generate arbitrary MBA identities:
\thm{Theorem 1}{Let $e$ be a linear MBA expression with the same variable definitions
as in Definition 2. Then let $(v_{0,j},...,v_{2^t-1, j})^T$ be the column vector
of the truth table of the Boolean expression from $e_j$, and let
$A = (v_{i,j})_{2^t \times s}$ be the $\B$-matrix of those truth tables over ($\Z/2^n\Z$).
Let $Y_{s \times 1} = (y_0, \dots, y_{s-1})^T$ be a vector of $s$ variables over
$(\Z/2^n\Z)$. Then $e = 0$ if and only if $AY = 0$ has a solution with $Y = (a_0, ..., a_s-1)^T$.}
{\textbf Proof:} Then, we need to prove that $AY = 0$ with $Y = (a_0, ..., a_{s-1})^T \implies e = 0$.
    Let $A Y = 0$ with $Y = (a_0, ..., a_{s-1})^T$. Let $f_j(x_{1,i}, ..., x_{t, i})$
    represent the bitwise expression for $e_j$ on the $i$th bits of $(x_1, ..., x_t)$,
    such that $e_j(x_1, ..., x_t) = \begin{bmatrix}
        f_j(x_{1,0}, ..., x_{t, 0})\\
        \vdots \\
        f_j(x_{1,n-1}, ..., x_{t, n-1})
    \end{bmatrix}$
    Now, we can write
    $e$ as:
    \begin{align*}
        \sum_{j=0}^{s-1}a_j \cdot e_j(x_1, ..., x_t) &= \sum_{j=0}^{s-1} a_j \cdot \begin{bmatrix}
        f_j(x_{1,0}, ..., x_{t, 0})\\
        \vdots \\
        f_j(x_{1,n-1}, ..., x_{t, n-1})
        \end{bmatrix}\\
                                                     &= \sum_{j=0}^{s-1} a_j \sum_{i=0}^{n-1} f_j(x_{1,i}, ..., x_{t, i}) \cdot 2^{i}\\
                                                     &= \sum_{i=0}^{n-1} 2^i
                                                     \left(\sum_{j=0}^{s-1} a_j
                                                     \cdot f_j(x_{1,i}, ...,
                                                     x_{t, i}) \right)
    \end{align*}
    Then, we know that since $AY = 0$, then
    \begin{align*}
        \sum_{j=0}^{s-1} a_j \cdot f_j(x_{1,i}, ..., x_{t, i}) = 0
    \end{align*}
    for all $i$, since $A$ represents the truth values of all of $f_j$, and thus
    \begin{align*}
        \sum_{j=0}^{s-1} a_j \sum_{i=0}^{n-1} f_j(x_{1,i}, ..., x_{t, i}) \cdot 2^{i} = 0
    \end{align*}
    and therefore $e = 0$. \qed
\par A lot of difficult notation and background goes into that proof, but
to highlight the important part, what this theorem says is that constructing
a matrix of truth tables $A$ from some functions $f_j$ on some bit vectors $(x_1, ..., x_t)$,
and then solving $AY = 0$ for $Y$ allows us to construct an MBA identity.
\par Let's take an example, consider that we wanted to compose an MBA identitity consisting
of the following expressions:
\begin{align*}
    f_0(x,y) &= x\\
    f_1(x,y) &= y\\
    f_2(x,y) &= x \land \neg y\\
    f_3(x,y) &= x \oplus y\\
    f_4(x,y) &= \neg x\\
    f_5(x,y) &= -1
\end{align*}
Then, we take the truth tables for each of these expressions in $\B$
and form them into a column vector, then compose them into a matrix.
In this case, we get the following:
\begin{align*}
    A =
    \begin{bmatrix}
        0 & 0 & 0 & 0 & 1 & 1\\
        0 & 1 & 0 & 1 & 1 & 1\\
        1 & 0 & 1 & 1 & 0 & 1\\
        1 & 1 & 0 & 0 & 0 & 1\\
    \end{bmatrix}
\end{align*}
(Where we've defined -1 to be all 1's as in 2's complement representation)
Then, solving for $Ax = 0$, we can take some solution:
\begin{align*}
    x = \begin{bmatrix}
    -1\\-1\\-2\\1\\-2\\2
    \end{bmatrix}
\end{align*}
So, plugging in our expressions from the functions the matrix was composed of,
we get the following equation:
\begin{align*}
    - x - y - 2(x \land \neg y) + (x \oplus y) - 2(\neg x) - 2 = 0
\end{align*}
So, we generated an MBA identity by just inputting some expressions we would like to
have seen in the final result. Great! Now, we did this by composing $A$ under the
assumption that all of our variables were 1-bit large. If we were working in the $n$-bit
space, $A$ would become much larger and be more difficult to find a solution for.
Luckily, there's another useful theorem here, which
we will describe the mathematical justification for, but not rigorously prove.
\thm{Theorem 2}{Let $e : (\B^1)^t \rightarrow \B^1$ be an MBA expression such that
    $e = 0$. Then, for any word-size $n$, $e : (\B^n)^t \rightarrow \B^n = 0$.
}
In other words, any MBA identity that holds in $\B$ also holds in $\B^n$.
To briefly look at why this is the case, consider the following in $\B^1$:
\begin{align*}
    e = \sum_{j=0}^{s-1}a_j \cdot e_j(x_1, ..., x_t) &= \sum_{j=0}^{s-1} a_j \cdot \begin{bmatrix}
    f_j(x_{1, 0}, ..., x_{t, 0})
    \end{bmatrix}\\
\end{align*}
Then, we know that for all $(x_{1,0}, ..., x_{t,0}) \in (\B^n)^t$, by our assumption $e = 0$,
then $f(x_{1,0}, ..., x_{t, 0}) = 0$. Extending this to the $n$-bit space:
\begin{align*}
    e = \sum_{j=0}^{s-1}a_j \cdot e_j(x_1, ..., x_t) &= \sum_{j=0}^{s-1} a_j \cdot \begin{bmatrix}
    f_j(x_{1,0}, ..., x_{t, 0})\\
    \vdots \\
    f_j(x_{1,n-1}, ..., x_{t, n-1})
    \end{bmatrix}\\
\end{align*}
Then:
\begin{align*}
    e =  \sum_{j=0}^{s-1} a_j \sum_{i=0}^{n-1} f_j(x_{1,i}, ..., x_{t, i}) \cdot 2^{i}
\end{align*}
By the above, we know that $f_j = 0$ when operating on singular bits, which is
exactly what
we do here, so:
\begin{align*}
    &= \sum_{j=0}^{s-1} a_j \sum_{i=0}^{n-1} 0 \cdot 2^{i} = 0
\end{align*}
Therefore, an MBA identity that holds in the 1-bit space holds in the $n$-bit space,
and we can use the results from these identities with any word size! This is
great for obfuscating our previous function $F$, which works with 32-bit numbers.
\subsection{Obfuscating Mixed Boolean Arithmetic}
Now, using the method in the previous section to generate MBA identities works well,
but we can also use it to find MBA equalities that allow us to rewrite our original
expression. Take the first example:
\begin{align*}
    &  - x - y - 2(x \land \neg y) + (x \oplus y) - 2(\neg x) - 2 = 0\\
    &\implies - x - y = 2(x \land \neg y) - (x \oplus y) + 2(\neg x) + 2\\
    &\implies x - y = 2(x \land \neg y) - (x \oplus y) + 2(\neg x) + 2 + 2x
\end{align*}
Then, in any MBA expression that contains $(x - y)$, we can replace it with
the expression on the right. Since we can generate arbitrary MBA identities,
we can compose any matrix that includes some of the operations in whichever
MBA expression we'd like to obfuscate, and then use the resulting equality
to replace expressions with equivalent ones as long as we'd like to! For example, let's
apply this MBA identity to our function $F$ by rewriting the $x-y$ term and calling
the obfuscated function $G$, and look at
the result in Python:
\\
\begin{minipage}{.5\textwidth}
    \begin{small}
    \begin{framed}
        \begin{verbatim}
def F(x: int, y: int,
      z: int) -> int:
    return 22*(x - y + ~z)



        \end{verbatim}
    \end{framed}
    \end{small}
\end{minipage}% This must go next to `\end{minipage}`
\begin{minipage}{.5\textwidth}
    \begin{framed}
        \begin{small}
        \begin{verbatim}
def G(x: int, y: int,
      z: int) -> int:
    return 22*(2*(x & ~y)
            - (x ^ y)
            + 2*(~x) + 2
            + 2*x + ~z)
        \end{verbatim}
        \end{small}
    \end{framed}
\end{minipage}
The expression on the right is much more difficult to understand for someone
reading the code after it's published or sold, and we could even repeat the process
several times to obfuscate with the $z$ term and make the expression look as complex
as we want. To ensure that our results are correct, we tested that the results of
these expressions are equivalent for several thousand random values and found that
it succeeded each test.
\par It's worthwhile to point out that obfuscating code arbitrarily is not a great idea,
as there are performance concerns once the expression starts getting too big, especially
if we expect to be using it very often. Therefore, it's a matter of cost-benefit
analysis to decide how much to use this kind of MBA obfuscation.
